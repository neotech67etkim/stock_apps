{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28e5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = '해보지 않으면 해낼 수 없다'\n",
    "result = text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765a1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다',\n",
    "       '텍스트의 단어를 토큰화해야 딥러닝에서 인식됩니다',\n",
    "       '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a2961b6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " 'fit_on_sequences',\n",
       " 'fit_on_texts',\n",
       " 'get_config',\n",
       " 'sequences_to_matrix',\n",
       " 'sequences_to_texts',\n",
       " 'sequences_to_texts_generator',\n",
       " 'texts_to_matrix',\n",
       " 'texts_to_sequences',\n",
       " 'texts_to_sequences_generator',\n",
       " 'to_json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38c4cde",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "먼저 텍스트의 각 단어를 나누어 토큰화합니다\n",
      "텍스트의 단어를 토큰화해야 딥러인에서 인식됩니다\n",
      "토큰화 한 결과는 딥러닝에서 사용할 수 있습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['먼저',\n",
       " '텍스트의',\n",
       " '각',\n",
       " '단어를',\n",
       " '나누어',\n",
       " '토큰화합니다',\n",
       " '텍스트의',\n",
       " '단어를',\n",
       " '토큰화해야',\n",
       " '딥러인에서',\n",
       " '인식됩니다',\n",
       " '토큰화',\n",
       " '한',\n",
       " '결과는',\n",
       " '딥러닝에서',\n",
       " '사용할',\n",
       " '수',\n",
       " '있습니다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= ''\n",
    "for i in docs:\n",
    "    print(i)\n",
    "    s = s+i\n",
    "    s=s+' '\n",
    "text_to_word_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab1c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer() 함수\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38b2741",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('먼저', 1),\n",
       "             ('텍스트의', 2),\n",
       "             ('각', 1),\n",
       "             ('단어를', 2),\n",
       "             ('나누어', 1),\n",
       "             ('토큰화합니다', 1),\n",
       "             ('토큰화해야', 1),\n",
       "             ('딥러인에서', 1),\n",
       "             ('인식됩니다', 1),\n",
       "             ('토큰화', 1),\n",
       "             ('한', 1),\n",
       "             ('결과는', 1),\n",
       "             ('딥러닝에서', 1),\n",
       "             ('사용할', 1),\n",
       "             ('수', 1),\n",
       "             ('있습니다', 1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa6024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'해보지 않으면 해낼 수 없다'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "498be300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15]]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences([text])\n",
    "print(x)\n",
    "\n",
    "xx = token.texts_to_matrix([text])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
